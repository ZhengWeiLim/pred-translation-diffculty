This document describes the pipeline setups for alignments, which has been tested by some preliminary experiments on
Malay-English Open subtitles data. The purpose of this is to review the processes and ensure the steps to be equally
applicable to Russian-English, French-English and Chinese-English.


@ Preprocesses

The subtitles are somewhat pre-aligned at the sentence level (mostly by frame/time stamps). Before aligning them by
words, the sentences across all languages are tokenized with nltk (except for Chinese, which is segmented from
CoreNLPParser), which distinguish space-separated tokens from punctuations. Upon closer examinations I discovered some
words containing characters not decodable by unicode scheme. I opted to discard these tokens -- most of them are
gibberish anyway. I also removed whitespace unicodes, specifically '\u200e' and '\u200b', which do not appear in prints,
but can be confusing when they are dealt with across different models.



@ Alignments

The word aligner comes from https://github.com/neulab/awesome-align, w/o --train_co (for better performance in our set of
languages). This outputs probabilities for each pair of word probabilities, which could be used as a filter for quality
control. Here a word can be aligned with multiple words, and are treated as one form of alignment. Likewise, because
timestamp-based alignments are susceptible to errors, I also employed a sentence aligner (from
https://tfhub.dev/google/LaBSE/2) to assign similarity scores to pre-aligned sentences for QC purposes.



@ Counting alignments

Some first/last words in subtitles starts with / ends with '-' characters, these mostly would not affect the alignments,
but they are cleaned and all words are lowercased for frequency and alignment counts. Frequency count is a simple count
of words that are split by empty space from tokenized sentences. For alignment counts, the first filter is based on
sentence alignment scores, where the threshold is very liberally set at >=0.3 (after a rough inspection, score is out of
1). This leaves 37424/1928054 (~2%) sentences out of the alignment counts. The next filter is by word alignment
probability, where the threshold is arbitrarily set at 0.4 (tbc), which drops 407242/11384535 (~3%) pairs of word
alignments (for comparison, the drop percentage becomes ~10% when alg_thresh=0.8 and ~0.001% when alg_thresh=0.2). Based
on these counts, I excluded alignments that only happen fewer than 5 times and where the source words occur fewer than
15 times. Finally, I removed alignments where counts account for < 1.5% over total counts of all possible alignments.
This is by far the most useful filter at ridding the alignment errors (i.e. the long tail) systematically, which builds
up for highly-frequent words and causes our variation measurements to fail (by overestimating their number of possible
alignments).



@ Pseudowords

First of all, why pseudowords? To keep me reminded of the idea down the line, I will quote Charles' Slack message:

"I thought the pseudoword idea gave us a way to evaluate different methods for identifying words with more alignments
than expected by chance. If you make pseudowords (like door - banana) they should have more different alignments than
comparable words with the same frequency. Let’s suppose that you create 100 different pseudowords (ideally that cover
different frequency bands). Suppose also that you have two candidate methods for ranking words based on how variable
their translations are. You can now compute the average rank assigned by each method to the pseudowords — the method
that assigns lower average rank to the pseudowords (assuming word 1 has the most variable translations) is probably
better."

and on the sampling method:

"... rank the words and to make bins based on these ranks — e.g. the rarest N words go in bin 1, the next rarest N go in
bin 2, and so on. That way you don’t have to worry about any specific parametric transformation."

Words are sorted by their frequency and distributed by bins of roughly equal size (max_bin_size -
min_bin_size = 1 to account for leftovers). I sampled 200 pseudowords uniformly (=400 words in total) from 20 bins
(10 pseudowords per bin). A pseudoword's frequency is the total frequency of its constituents, and its alignments a
combination of its constituents' alignments.



@ Morphology analysis

We also consider the morphology of words -- given that Malay and English have (unsurprisingly) different systems of
inflection, we want to match their morphemes as well as we possibly could. For Malay, the analyser comes from
https://github.com/matbahasa/MALINDO_Morph. This allows prefixes and suffixes to be split from the words' lemmas. Malay
also duplicate forms (kata ganda) to show quantity, and the analyser will show if it is one of "R-penuh"/"R-separa"/
"R-ritma", depending on the duplicates' types. This information can be matched with English "-s" suffix for plural
nouns. On the English side, I only lemmatize a word if it is one of verbs, nouns, adjectives or adverbs, and include
their suffixes accordingly (by max-matching the words) for alignments. This is specifically to restrict change of forms
for auxiliary verbs (e.g., has, was), which do not have any equivalents in Malay. Note that, apart for Malay, spaCy
package covers every other languages we are interested in for morphology analysis and POS tagging. Now that I have found
a good enough analyser and a tagger for Malay, there should not be any more problems when we expand the procedure to
other languages.



@ Selecting words by POS

I use spaCy to tag English words. Malay doesn't have any other reliable and ready-to-use taggers apart from
https://github.com/indolem/indolem/tree/main/pos_tagging, that's primarily trained for Indonesian (and they use
Penn-Treebank-style tags). I took a few looks on the tagged Malay words, and they seem good to me. The goal of this is
to retrieve something like "the top X nouns that has the most translation variety in Malay/English". We also postulate
that while lemmatized forms highlight words that are culturally important, tagged words capture cultural specificity --
these are still conjectures, we will need more robust evidence/test (where direction of translations can be controlled),
and should further examine this with other language pairs.


